{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# from category_encoders import *\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "# from MLP import *\n",
    "import networkx as nx\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "sys.path.append('../data/pengpai/')\n",
    "import address\n",
    "from bidict import bidict\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from dataloader import get_dataloader\n",
    "import argparse\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "HEAD = 0\n",
    "TAIL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', type=str, default='att_model', help='model name ')\n",
    "parser.add_argument('--lr_LP', type=float, default=18e-5, help='learning rate')\n",
    "# parser.add_argument('--lr_path', type=float, default=1e-4, help='learning rate')\n",
    "parser.add_argument('--ratio', type=float, default=1.0, help='ratio of neg samples and pos samples')\n",
    "parser.add_argument('--num_epochs', type=int, default=100, help='number of epochs')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=128, help=\"LP batch size\")\n",
    "parser.add_argument(\"--model_save_dir\", type=str, default=\"saved/model/MLP\", help=\"save dir for model\")\n",
    "parser.add_argument(\"--HEAD\", type=int, default=0, help=\"\")\n",
    "parser.add_argument(\"--TAIL\", type=int, default=1, help=\"\")\n",
    "parser.add_argument(\"--LABEL\", type=int, default=2, help=\"\")\n",
    "parser.add_argument(\"--upperboundratio_10time\", type=int, default=2,\n",
    "                    help=\"upperboundratio_10time for ratio analysis\")\n",
    "parser.add_argument(\"--sample_type\", type=str, default='random',\n",
    "                    help=\"sample type for pos and neg case pairs\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=2, help=\"gpu device\")\n",
    "parser.add_argument(\"--proportion\", type=float, default=1.0, help=\"proportion of the total data for train and test\")\n",
    "# parser.add_argument(\"--importance_threshold\", type=float, default=0.2, help=\"place importance threshold\")\n",
    "# parser.add_argument(\"--classification_num_epoch\", type=int, default=100, help=\"number for classification epochs\")\n",
    "# parser.add_argument(\"--classification_lr\", type=float, default=1e-4, help='classification learning rate')\n",
    "parser.add_argument(\"--run_model\", type=str, default='ablation')\n",
    "# parser.add_argument(\"--load_epo\",type=int,default=-1)\n",
    "parser.add_argument(\"--lbd\", type = int,default=4)\n",
    "parser.add_argument(\"--get_best\", type=bool, default=False)\n",
    "parser.add_argument(\"--epo\", type=int, default=-1)\n",
    "parser.add_argument('--path_len',type = int, default= 45)\n",
    "parser.add_argument(\"--num_gb_epoch\",type=int,default=20,help=\"number of epoch for geting best model\")\n",
    "parser.add_argument('--rl_name',type = str,default='hprl_carl')\n",
    "parser.add_argument('--baseline_save_path',type = str,default = 'saved/baselines')\n",
    "# parser.add_argument(\"--\")\n",
    "# parser.add_argument(\"--lbd_sen\",type=bool,default=False,help = \"Sensitivity analysis for lbd\")\n",
    "config = parser.parse_args(args=[])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindexed_sp_net, old2new_case_id, reindexed_case_tensor_padded = get_case_tensor('gne')\n",
    "# train_dataloader, test_dataloader, all_dataloader = get_dataloader(config, reindexed_sp_net, old2new_case_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_idx_pair_train = train_dataloader.dataset.dataset.data\n",
    "# case_idx_pair_test = test_dataloader.dataset.dataset.data\n",
    "# case_idx_pair_train = load_file('dataset/case_idx_pair/random/train_4.pickle')\n",
    "# case_idx_pair_test = load_file('dataset/case_idx_pair/random/test_4.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_idx_pair_train = np.array(train_dataloader.dataset.dataset)[train_dataloader.dataset.indices]\n",
    "# case_idx_pair_test = np.array(train_dataloader.dataset.dataset)[test_dataloader.dataset.indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_idx_pair_train.dataset.data\n",
    "def get_train_test_np(case_idx_pair,reindexed_case_tensor_padded):\n",
    "    case_pair_tensor=[]\n",
    "    for (case_i,case_j,lab) in case_idx_pair:\n",
    "        emb = []\n",
    "        emb += reindexed_case_tensor_padded[case_i].tolist()\n",
    "        emb += reindexed_case_tensor_padded[case_j].tolist()\n",
    "        emb += [lab]\n",
    "        case_pair_tensor+=[emb]\n",
    "    return np.array(case_pair_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_tensor_pair_train = get_train_test_np(case_idx_pair_train,reindexed_case_tensor_padded)\n",
    "# case_tensor_pair_test = get_train_test_np(case_idx_pair_test,reindexed_case_tensor_padded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_tensor_pair_train.shape == case_tensor_pair_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case_tensor_pair_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def get_train_test(model_name):\n",
    "    reindexed_sp_net, old2new_case_id, reindexed_case_tensor_padded = get_case_tensor(model_name)\n",
    "    train_dataloader, test_dataloader, all_dataloader = get_dataloader(config, reindexed_sp_net, old2new_case_id)\n",
    "    case_idx_pair_train = load_file('dataset/case_idx_pair/random/train_4.pickle')\n",
    "    case_idx_pair_test = load_file('dataset/case_idx_pair/random/test_4.pickle')\n",
    "    case_tensor_pair_train = get_train_test_np(case_idx_pair_train,reindexed_case_tensor_padded)\n",
    "    case_tensor_pair_test = get_train_test_np(case_idx_pair_test,reindexed_case_tensor_padded)\n",
    "    tensor_train = shuffle(case_tensor_pair_train)\n",
    "    X_train = tensor_train[:,:-1]\n",
    "    y_train = tensor_train[:,-1]\n",
    "\n",
    "    tensor_test = shuffle(case_tensor_pair_test)\n",
    "    X_test = tensor_test[:,:-1]\n",
    "    y_test = tensor_test[:,-1]\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model_name):\n",
    "    \n",
    "    X_train,y_train,X_test,y_test = get_train_test(model_name)\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.fit(X_train,y_train)\n",
    "    y_test,y_pred =  y_test, model.predict(X_test)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy : %.4g\" % accuracy) \n",
    "    precision = metrics.precision_score(y_test,y_pred)\n",
    "    print('Precision: %.4g' % precision)\n",
    "    recall = metrics.recall_score(y_test,y_pred)\n",
    "    print('Recall: %.4g' % metrics.recall_score(y_test,y_pred))\n",
    "    f1 = metrics.f1_score(y_test,y_pred)\n",
    "    print('F1: %.4g' % f1)\n",
    "\n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "    print(\"AUC Score (Test): %.4g\" % metrics.roc_auc_score(y_test, y_pred))\n",
    "    return (y_test,y_pred,accuracy,precision,recall,f1,auc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model_name,accuracy,precision,recall,f1,auc')\n",
    "model_list = ['hprl_carl']\n",
    "res = load_file('saved/results/XGBoost.pickle')\n",
    "for model_name in model_list:\n",
    "    print('train on:{}'.format(model_name))\n",
    "    res[model_name] = train_test(model_name)\n",
    "    save_pickle(obj = res, filepath='saved/results/XGBoost.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res['hprl_carl']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ced651eb8c2205c8626fa5bb71c2d74f7a3a5427357a517533210808900200ae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}