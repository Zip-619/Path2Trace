{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文件说明\n",
    "根据case_path使用图表征方法获得place的表征，city inmigration通过CAE方法获得了case path中place的表征，最后将这两个表征通过AE方法获得最后的place的表征\n",
    "得到以下文件：\n",
    "1. poi_concatenate_embedding.pickle,将各层的embedding拼接后得到poi最后得到的向量字典，格式如下：\n",
    "\n",
    "        {\n",
    "            poiaddr1:emb1\n",
    "            poiaddr2:emb2\n",
    "        }\n",
    "\n",
    "2. city_poi_emb_dict.pickle, 将city的embedding和POI的embedding进行拼接\n",
    "3. autoae_poi_emb_dict.pickle, 经过autoAE之后的每个place的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import difflib\n",
    "import torch\n",
    "import networkx as nx\n",
    "\n",
    "sys.path.append('../data/pengpai')\n",
    "import address\n",
    "sys.path.append('../')\n",
    "\n",
    "from argparse import *\n",
    "from deepwalkpytorch.deepwalk import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_data_base = '../data/pengpai/labeled_data/idx_address_network_no_edgedata/'\n",
    "levels = ['poi','village','township','county','city','province','full']\n",
    "level2embeddingsize = {'full':120,'province':4,'city':8,'county':12,'township':16,'village':32,'poi':48,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层次化地点表征 （HPRL）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_init():\n",
    "    # deepwalk 参数设置\n",
    "    parser = ArgumentParser(\"deepwalk\",\n",
    "                            formatter_class=ArgumentDefaultsHelpFormatter,\n",
    "                            conflict_handler='resolve')\n",
    "\n",
    "    parser.add_argument(\"--debug\", dest=\"debug\", action='store_true', default=False,\n",
    "                        help=\"drop a debugger if an exception is raised.\")\n",
    "\n",
    "    parser.add_argument('--format', default='edgelist',\n",
    "                        help='File format of input file')\n",
    "\n",
    "    parser.add_argument('--input', nargs='?', required=True,\n",
    "                        help='Input graph file')\n",
    "\n",
    "    parser.add_argument(\"-l\", \"--log\", dest=\"log\", default=\"INFO\",\n",
    "                        help=\"log verbosity level\")\n",
    "\n",
    "    parser.add_argument('--matfile-variable-name', default='network',\n",
    "                        help='variable name of adjacency matrix inside a .mat file.')\n",
    "\n",
    "    parser.add_argument('--max-memory-data-size', default=1000000000, type=int,\n",
    "                        help='Size to start dumping walks to disk, instead of keeping them in memory.')\n",
    "\n",
    "    parser.add_argument('--number-walks', default=10, type=int,\n",
    "                        help='Number of random walks to start at each node')\n",
    "\n",
    "    parser.add_argument('--output', required=True,\n",
    "                        help='Output representation file')\n",
    "\n",
    "    parser.add_argument('--representation-size', default=64, type=int,\n",
    "                        help='Number of latent dimensions to learn for each node.')\n",
    "\n",
    "    parser.add_argument('--seed', default=0, type=int,\n",
    "                        help='Seed for random walk generator.')\n",
    "\n",
    "    parser.add_argument('--undirected', default=True, type=bool,\n",
    "                        help='Treat graph as undirected.')\n",
    "\n",
    "    parser.add_argument('--vertex-freq-degree', default=False, action='store_true',\n",
    "                        help='Use vertex degree to estimate the frequency of nodes '\n",
    "                            'in the random walks. This option is faster than '\n",
    "                            'calculating the vocabulary.')\n",
    "\n",
    "    parser.add_argument('--walk-length', default=40, type=int,\n",
    "                        help='Length of the random walk started at each node')\n",
    "\n",
    "    parser.add_argument('--window-size', default=5, type=int,\n",
    "                        help='Window size of skipgram model.')\n",
    "\n",
    "    parser.add_argument('--workers', default=1, type=int,\n",
    "                        help='Number of parallel processes.')\n",
    "    \n",
    "    return parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(idx_data_base,'poi'+'_net_edgelist_intnode.txt')\n",
    "g = nx.read_edgelist(input_path,nodetype = int)  # 读取edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_parser(level):\n",
    "    input = os.path.join(idx_data_base,level+'_net_edgelist_intnode.txt')\n",
    "    output = 'levels_embeddings/'+level+'embeddings'\n",
    "    p = parser_init()\n",
    "    parser = p.parse_args(['--input',input,'--output',output,'--representation-size',str(level2embeddingsize[level])])\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分层处理 层次化embedding\n",
    "# 分层dw\n",
    "def level_dw():\n",
    "    for level in levels:\n",
    "    # level = 'poi'\n",
    "        if level == 'poi':\n",
    "            continue\n",
    "        input = os.path.join(idx_data_base,level,'_net_edgelist_intnode.txt')\n",
    "        parser = level_parser(level)\n",
    "        dw.process(parser)\n",
    "        print(level+' representation learning done!')\n",
    "        # data/pengpai/labeled_data/idx_address_network_no_edgedata/poi_net_edgelist_intnode.txt\n",
    "    print('All levels representation learning done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embdict(embedding):\n",
    "    \"\"\"\n",
    "    获得 地点与embedding 的对应关系\n",
    "    \"\"\"\n",
    "    with open('../data/pengpai/labeled_data/idx_address_network_no_edgedata/nodeidxmaps.pickle','rb') as file:\n",
    "        addr2idxmaps = pickle.load(file)\n",
    "        file.close()\n",
    "    addr2idx = addr2idxmaps['poi']\n",
    "    idx2addr = {val:key for key,val in addr2idx.items()}\n",
    "    addr2emb = {}\n",
    "    for i in range(len(embedding)):\n",
    "        addr2emb[idx2addr[i]] = embedding[i]\n",
    "    return addr2emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('../data/pengpai/labeled/idx_address_network_no_edgedata/nodeidxmaps.pickle','rb') as file:\n",
    "    addr2idxmaps = pickle.load(file)\n",
    "    file.close()\n",
    "with open('../data/pengpai/labeled/case_path.pickle','rb') as file:\n",
    "    case_path = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得不同层次的embedding和address的对应关系，广播机制被应用其中\n",
    "addr2embedding = {}\n",
    "for level in levels:\n",
    "    embeddings = np.load('levels_embeddings/'+level+'embeddings.npy')\n",
    "    addr2idx =  addr2idxmaps[level]\n",
    "    addr2embedding[level] = {addr:embeddings[idx] for addr,idx in addr2idx.items()} # 广播机制应用其中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_sim_addr(address,level):\n",
    "    # 在level中的地点中找到与address最相近的地址\n",
    "    word = ''\n",
    "    sim = 0\n",
    "    max = 0\n",
    "    for addr in addr2embedding[level].keys():\n",
    "        sim = difflib.SequenceMatcher(None,addr,address).quick_ratio()\n",
    "        if sim > max:\n",
    "            max = sim\n",
    "            word = addr\n",
    "\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将不同层次的地点表征拼接\n",
    "def get_hprl_con_emb():\n",
    "    final_embedding = {}\n",
    "    # for level in levels: # 不同level的图\n",
    "    count = 0\n",
    "    for addr in list(addr2embedding['poi'].keys()): # 该等级的图的地点\n",
    "        final_embedding[addr] = np.array([],dtype=float)\n",
    "        for level in levels:\n",
    "            # find_exact = False\n",
    "            if (level == 'full'):\n",
    "                continue\n",
    "            try:\n",
    "                final_embedding[addr] = np.concatenate((final_embedding[addr],addr2embedding[level][addr]))\n",
    "            except KeyError:\n",
    "                word = find_most_sim_addr(addr,level)\n",
    "                if word != '':\n",
    "                    final_embedding[addr] = np.concatenate((final_embedding[addr],addr2embedding[level][word]))\n",
    "                else:\n",
    "                    count+=1\n",
    "                    final_embedding[addr] = np.concatenate((final_embedding[addr],np.random.random((level2embeddingsize[level]))))\n",
    "    poi_embedding = final_embedding\n",
    "    return poi_embedding\n",
    "poi_concatenate_embedding = get_hprl_con_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poi_concatenate_embedding.pickle','rb') as file:\n",
    "    poi_concatenate_embedding = pickle.load(file=file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载城市的表征（CARL）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载城市表征\n",
    "def get_CARL_emb():\n",
    "    city_img_mvin_emb = np.fromfile('../data/city_inmigration/data/move_in/2020-01-19/weighted_1.emb',dtype=np.float32)\n",
    "    city_img_mvout_emb = np.fromfile('../data/city_inmigration/data/move_out/2020-01-19/weighted_1.emb',dtype=np.float32)\n",
    "    city_img_mvin_emb = city_img_mvin_emb.reshape((-1,64))\n",
    "    city_img_mvout_emb = city_img_mvout_emb.reshape((-1,64))\n",
    "    city_img_emb = np.concatenate([city_img_mvin_emb,city_img_mvout_emb],axis=1)\n",
    "    return city_img_emb\n",
    "city_img_emb = get_CARL_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将HPRL地点的表征与城市的表征拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_poi_concat(city_emb,poi_embedding):\n",
    "\n",
    "    with open('../data/city_inmigration/data/city2idx.pickle','rb') as file:\n",
    "        city2idx = pickle.load(file)\n",
    "        file.close()\n",
    "    city_poi_emb_dict = {}\n",
    "    count = 0\n",
    "    for poi_addr,poi_emb in poi_embedding.items():\n",
    "        got_city = 0\n",
    "        for city,idx in city2idx.items():\n",
    "            if city in poi_addr: # 有些只有省，那么对于那些只有省的就可以丢弃了？\n",
    "                got_city = 1\n",
    "                city_poi_emb_dict[poi_addr] = np.concatenate([city_emb[city2idx[city]],poi_embedding[poi_addr]])\n",
    "        if got_city==0: # 没找到的话用随机的一个128维的向量代替cityemb\n",
    "            count += 1 \n",
    "            city_poi_emb_dict[poi_addr] = np.concatenate([np.random.random(128),poi_embedding[poi_addr]])\n",
    "    return city_poi_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPRL_CARL = city_poi_concat(city_img_emb,poi_concatenate_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_autoencoder_dict(concat_emb,patience):\n",
    "    refined_input_data,seq_len = core.prepare_dataset(np.array(list(concat_emb.values())))\n",
    "    encoded,decoded,final_loss = core.QuickEncode(refined_input_data,embedding_dim=128,patience = patience)\n",
    "    i=0\n",
    "    enc_emb = dict()\n",
    "    for poi_addr in concat_emb.keys():\n",
    "        enc_emb[poi_addr] = np.array(encoded[i])\n",
    "        i+=1\n",
    "    return enc_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拼接后的表征通过autoencoder降维融合处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Autoencoder import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 100, loss_sum : 116631.1718750\n",
      "epoch : 200, loss_sum : 85295.8359375\n",
      "epoch : 300, loss_sum : 68704.3828125\n",
      "epoch : 400, loss_sum : 59088.7304688\n",
      "epoch : 500, loss_sum : 52372.1835938\n",
      "epoch : 600, loss_sum : 47671.8828125\n",
      "epoch : 700, loss_sum : 43041.4492188\n",
      "epoch : 800, loss_sum : 39330.9648438\n",
      "epoch : 900, loss_sum : 35928.2812500\n",
      "epoch : 1000, loss_sum : 34721.2851562\n",
      "Early Stopping activated. Final validation loss : 33248.2617188\n"
     ]
    }
   ],
   "source": [
    "HPRL_CARL = get_autoencoder_dict(HPRL_CARL,patience=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# represatation learning done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poi_embeddings/hprl_carl_poi_embedding.pickle','wb') as file:\n",
    "    pickle.dump(file=file,obj = HPRL_CARL)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation EXP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9304/1082385564.py\u001B[0m in \u001B[0;36mget_hprl_con_emb\u001B[1;34m()\u001B[0m\n\u001B[0;32m     12\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m                 \u001B[0mfinal_embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal_embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maddr2embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: '河北省唐山市遵化市西留村乡村卫生所'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9304/802011995.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpoi_concatenate_embedding\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_hprl_con_emb\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mHPRL\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_autoencoder_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9304/1082385564.py\u001B[0m in \u001B[0;36mget_hprl_con_emb\u001B[1;34m()\u001B[0m\n\u001B[0;32m     13\u001B[0m                 \u001B[0mfinal_embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal_embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maddr2embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m                 \u001B[0mword\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfind_most_sim_addr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mword\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;34m''\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m                     \u001B[0mfinal_embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinal_embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maddr2embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9304/4277042291.py\u001B[0m in \u001B[0;36mfind_most_sim_addr\u001B[1;34m(address, level)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0maddr\u001B[0m \u001B[1;32min\u001B[0m \u001B[0maddr2embedding\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[0msim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdifflib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSequenceMatcher\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maddr\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0maddress\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mquick_ratio\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0msim\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mmax\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m             \u001B[0mmax\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msim\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m             \u001B[0mword\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0maddr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "poi_concatenate_embedding = get_hprl_con_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\Case_Association_Prediction_opensource\\represent_learning\\..\\Autoencoder\\core.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_in_tensor = torch.tensor(sequential_data, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 100, loss_sum : 60608.3906250\n",
      "epoch : 200, loss_sum : 37221.5156250\n",
      "epoch : 300, loss_sum : 25889.2480469\n",
      "epoch : 400, loss_sum : 19105.4785156\n",
      "epoch : 500, loss_sum : 14824.8564453\n",
      "epoch : 600, loss_sum : 11998.0468750\n",
      "epoch : 700, loss_sum : 10460.7089844\n",
      "epoch : 800, loss_sum : 8345.3886719\n",
      "epoch : 900, loss_sum : 7654.8154297\n",
      "Early Stopping activated. Final validation loss : 7291.1884766\n"
     ]
    }
   ],
   "source": [
    "HPRL = get_autoencoder_dict(poi_concatenate_embedding,patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poi_embeddings/hprl_poi_embedding.pickle','wb') as file:\n",
    "    pickle.dump(file=file,obj=HPRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/pengpai/labeled_data/poi_log_lat_embedding.pickle','rb') as file:\n",
    "    poi_log_lat_embedding = pickle.load(file)\n",
    "\n",
    "with open('poi_embeddings/loc_poi_embedding.pickle','wb') as file:\n",
    "    pickle.dump(file=file,obj=poi_log_lat_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poi_embeddings/loc_poi_embedding.pickle','rb') as file:\n",
    "    a = pickle.load(file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CARL_LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\Case_Association_Prediction_opensource\\represent_learning\\..\\Autoencoder\\core.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_in_tensor = torch.tensor(sequential_data, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 100, loss_sum : 407557.8125000\n",
      "epoch : 200, loss_sum : 389230.9062500\n",
      "epoch : 300, loss_sum : 287570.5625000\n",
      "epoch : 400, loss_sum : 109389.5312500\n",
      "epoch : 500, loss_sum : 112824.1953125\n",
      "Early Stopping activated. Final validation loss : 100848.5625000\n"
     ]
    }
   ],
   "source": [
    "CARL_LOC = city_poi_concat(city_img_emb,poi_log_lat_embedding)\n",
    "CARL_LOC = get_autoencoder_dict(CARL_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poi_embeddings/carl_loc_poi_embedding.pickle','wb') as file:\n",
    "    pickle.dump(file=file,obj = CARL_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CARL_LOC['广西壮族自治区桂林市叠彩区'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from karateclub import DeepWalk,RandNE,GraRep,GLEE,NodeSketch,Node2Vec,BoostNE,HOPE\n",
    "\n",
    "m_dic = {'deepwalk':DeepWalk,'randne':RandNE,'grarep':GraRep,'nodesketch':NodeSketch,\n",
    "         'grarep':GraRep,'node2vec':Node2Vec,'boostne':BoostNE,'hope':HOPE}\n",
    "\n",
    "def get_save_emb(model_name,net):\n",
    "    # 根据不同的模型对poi层次的网络进行表征学习作为baseline\n",
    "#     dimensions = 128\n",
    "    if model_name == 'boostne':\n",
    "        model = m_dic[model_name](iterations = 15)\n",
    "    if model_name == 'nodesketch':\n",
    "        model = m_dic[model_name](iterations = 20,dimensions = dimensions)\n",
    "    else:\n",
    "        model = m_dic[model_name]()\n",
    "    print('training ...')\n",
    "\n",
    "    model.fit(net)\n",
    "    embedding = model.get_embedding()\n",
    "    embedding = get_embdict(embedding)\n",
    "    save_path = 'poi_embeddings/'\n",
    "    if os.path.exists(save_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(save_path)\n",
    "        \n",
    "    with open(os.path.join(save_path,model_name+'_poi_embedding.pickle'),'wb') as file:\n",
    "        pickle.dump(file = file, obj = embedding)\n",
    "        file.close()\n",
    "    print('saved')\n",
    "\n",
    "get_save_emb('hope',g)\n",
    "\n",
    "get_save_emb('nodesketch',g)\n",
    "\n",
    "get_save_emb('node2vec',g)\n",
    "\n",
    "# 图规模太大，无法训练\n",
    "get_save_emb('grarep',g)\n",
    "\n",
    "get_save_emb('deepwalk',g)\n",
    "\n",
    "get_save_emb('randne',g)\n",
    "\n",
    "get_save_emb('grarep',g)\n",
    "\n",
    "get_save_emb('boostne',g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得地点的类型，得到idx2type的字典并将其保存为np array\n",
    "def get_address_idx2type():\n",
    "    addrset = {}\n",
    "\n",
    "    # 获得不同level的地址\n",
    "    for level in levels:\n",
    "        addrset[level] = set()\n",
    "        for case,path in case_path.items():\n",
    "            for addr in path:\n",
    "                if addr.get_addr(level) == '':\n",
    "                    continue\n",
    "                addrset[level].add(addr.get_addr(level))\n",
    "\n",
    "\n",
    "    addr2type = {}\n",
    "\n",
    "    for case, path in case_path.items():\n",
    "        for addr in path:\n",
    "            for level in levels[:-1]:\n",
    "                if addr.get_addr(level) == addr.get_addr('full'):\n",
    "                    addr2type[addr.get_addr('full')]=level\n",
    "\n",
    "    idx2type = {}\n",
    "    for addr,idx in addr2idxmaps['poi'].items():\n",
    "        try:\n",
    "            idx2type[idx] = addr2type[addr]\n",
    "        except:\n",
    "            idx2type[idx] = 'poi'\n",
    "\n",
    "    def one_hot(length,idx):\n",
    "        arr = np.zeros(length)\n",
    "        arr[idx]=1\n",
    "        return arr\n",
    "\n",
    "    typemap = {'province':0,'city':1,'county':2,'township':3,'village':4,'poi':5}\n",
    "    for key,val in idx2type.items():\n",
    "        idx2type[key] = one_hot(len(typemap),typemap[val])\n",
    "\n",
    "    np.savetxt(os.path.join(idx_data_base,'poi'+'_feature.txt'),np.array(list(idx2type.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获得网络的树状层次结构，用于层次化的baseline\n",
    "def get_net_tree():\n",
    "    \n",
    "    tree = nx.DiGraph()\n",
    "\n",
    "    def start_with(pro,addr):\n",
    "        flag = True\n",
    "        for i in range(min(len(pro),len(addr))):\n",
    "            if pro[i] == addr[i]:\n",
    "                continue\n",
    "            else:\n",
    "                flag = False\n",
    "                break\n",
    "        return flag\n",
    "\n",
    "    flag = False\n",
    "    for level in levels:\n",
    "        if level == 'province':\n",
    "            for addr in addrset[level]:\n",
    "                tree.add_edge(10275,pro2idx[addr])\n",
    "        if level == 'poi':\n",
    "            for addr in addrset[level]:\n",
    "                flag = False\n",
    "                for pro in addrset['province']:\n",
    "                    if pro in addr:\n",
    "                        tree.add_edge(pro2idx[pro],addr2idxmaps[level][addr])\n",
    "                        flag = True\n",
    "                        break\n",
    "                if not flag:\n",
    "                    print(addr)\n",
    "                    tree.add_edge(pro2idx['黑龙江省'],addr2idxmaps[level][addr])\n",
    "    nx.write_edgelist(tree,'node_tree.txt')\n",
    "    return tree\n",
    "                    \n",
    "# tree.add_edge()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ced651eb8c2205c8626fa5bb71c2d74f7a3a5427357a517533210808900200ae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}